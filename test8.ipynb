{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1d5d8f0",
   "metadata": {},
   "source": [
    "# Facebook Staff Data Scientist Python Test\n",
    "\n",
    "This notebook is designed to evaluate your expertise in data science, machine learning, and ML Ops for a Staff Data Scientist position at Facebook. In this test, you will work with a synthetic dataset simulating user sessions on Facebook. Your tasks include data ingestion, preprocessing, exploratory data analysis (EDA), feature engineering, model building, and model tuning. In addition, you will answer questions regarding ML Ops practices for deploying and maintaining machine learning systems in production.\n",
    "\n",
    "Please read the instructions carefully and add your answers to the ML Ops questions in markdown cells at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0c8e87",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "You are provided with a synthetic dataset named **`facebook_data.csv`** that simulates user sessions on Facebook. The dataset includes the following columns:\n",
    "\n",
    "- **session_id:** Unique identifier for each session.\n",
    "- **user_id:** Unique identifier for each user.\n",
    "- **timestamp:** Timestamp of the session event.\n",
    "- **device_type:** Device used by the user (e.g., \"Desktop\", \"Mobile\", \"Tablet\").\n",
    "- **action:** The action performed by the user during the session (e.g., \"view\", \"like\", \"comment\", \"share\", \"post\").\n",
    "- **time_spent:** Duration of the session in seconds.\n",
    "- **content_category:** Category of the content viewed (e.g., \"news\", \"entertainment\", \"sports\", \"technology\", \"lifestyle\").\n",
    "- **engagement:** Binary indicator (0 or 1) representing whether the session resulted in significant engagement (e.g., a like, comment, share, or post).\n",
    "\n",
    "Your objective is to build a predictive model to estimate the likelihood of engagement based on session data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e5f3ad",
   "metadata": {},
   "source": [
    "## Dataset Generation\n",
    "\n",
    "Run the cell below to generate the synthetic dataset and save it as **`facebook_data.csv`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "214c7eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'facebook_data.csv' generated successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of sessions to simulate\n",
    "num_sessions = 1500\n",
    "\n",
    "# Generate unique session IDs\n",
    "session_ids = np.arange(1, num_sessions + 1)\n",
    "\n",
    "# Generate random user IDs (simulate repeated users)\n",
    "user_ids = np.random.randint(1, 1000, size=num_sessions)\n",
    "\n",
    "# Generate random timestamps within the year 2023\n",
    "base_date = datetime(2023, 1, 1)\n",
    "random_days = np.random.randint(0, 365, size=num_sessions)\n",
    "random_seconds = np.random.randint(0, 86400, size=num_sessions)\n",
    "timestamps = [base_date + timedelta(days=int(d), seconds=int(s)) for d, s in zip(random_days, random_seconds)]\n",
    "\n",
    "# Define device types with weighted probabilities\n",
    "device_types = np.random.choice(\n",
    "    ['Desktop', 'Mobile', 'Tablet'],\n",
    "    size=num_sessions,\n",
    "    p=[0.4, 0.5, 0.1]\n",
    ")\n",
    "\n",
    "# Define possible user actions with weighted probabilities\n",
    "actions = np.random.choice(\n",
    "    ['view', 'like', 'comment', 'share', 'post'],\n",
    "    size=num_sessions,\n",
    "    p=[0.5, 0.2, 0.1, 0.1, 0.1]\n",
    ")\n",
    "\n",
    "# Generate random time spent between 10 and 600 seconds\n",
    "time_spent = np.round(np.random.uniform(10, 600, size=num_sessions), 1)\n",
    "\n",
    "# Define content categories\n",
    "content_categories = np.random.choice(\n",
    "    ['news', 'entertainment', 'sports', 'technology', 'lifestyle'],\n",
    "    size=num_sessions\n",
    ")\n",
    "\n",
    "# Generate engagement indicator:\n",
    "# - If action is one of \"like\", \"comment\", \"share\", or \"post\", then engagement = 1.\n",
    "# - Otherwise (\"view\"), assign engagement = 1 with a 10% chance.\n",
    "engagement = []\n",
    "for act in actions:\n",
    "    if act in ['like', 'comment', 'share', 'post']:\n",
    "        engagement.append(1)\n",
    "    else:  # view\n",
    "        engagement.append(1 if np.random.rand() < 0.1 else 0)\n",
    "engagement = np.array(engagement)\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'session_id': session_ids,\n",
    "    'user_id': user_ids,\n",
    "    'timestamp': timestamps,\n",
    "    'device_type': device_types,\n",
    "    'action': actions,\n",
    "    'time_spent': time_spent,\n",
    "    'content_category': content_categories,\n",
    "    'engagement': engagement\n",
    "})\n",
    "\n",
    "# Save the dataset to a CSV file\n",
    "df.to_csv('facebook_data.csv', index=False)\n",
    "print(\"Dataset 'facebook_data.csv' generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e9c0bf",
   "metadata": {},
   "source": [
    "## Task 1: Data Loading and Preprocessing\n",
    "\n",
    "1. **Load the Data:**\n",
    "   - Read the `facebook_data.csv` file into a pandas DataFrame.\n",
    "   - Display the first few rows to understand the structure of the data.\n",
    "\n",
    "2. **Data Cleaning:**\n",
    "   - Check for and handle missing values and outliers.\n",
    "   - Convert the `timestamp` column to a datetime object.\n",
    "   - Create additional time-based features (e.g., hour of day, day of week)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a35d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('facebook_data.csv')\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Convert timestamp to datetime and create time-based features\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "data['hour'] = data['timestamp'].dt.hour\n",
    "data['day_of_week'] = data['timestamp'].dt.dayofweek\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Display data types\n",
    "print(\"\\nData types:\")\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac0d795",
   "metadata": {},
   "source": [
    "## Task 2: Exploratory Data Analysis (EDA)\n",
    "\n",
    "1. **Summary Statistics:**\n",
    "   - Compute descriptive statistics for numerical and categorical features.\n",
    "\n",
    "2. **Visualization:**\n",
    "   - Plot the distribution of key features (e.g., `time_spent`, `engagement`).\n",
    "   - Visualize relationships between features (e.g., device type vs. engagement, content category vs. time spent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e348e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Summary statistics\n",
    "print(\"Summary statistics:\")\n",
    "print(data.describe())\n",
    "\n",
    "# Distribution of engagement\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='engagement', data=data)\n",
    "plt.title(\"Distribution of Engagement\")\n",
    "plt.xlabel(\"Engagement (0 = No, 1 = Yes)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Distribution of time_spent\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(data['time_spent'], bins=30, kde=True)\n",
    "plt.title(\"Time Spent Distribution\")\n",
    "plt.xlabel(\"Time Spent (seconds)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Device type vs engagement\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(x='device_type', hue='engagement', data=data)\n",
    "plt.title(\"Device Type vs Engagement\")\n",
    "plt.xlabel(\"Device Type\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Content category vs time spent\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(x='content_category', y='time_spent', data=data)\n",
    "plt.title(\"Time Spent by Content Category\")\n",
    "plt.xlabel(\"Content Category\")\n",
    "plt.ylabel(\"Time Spent (seconds)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7a9a0c",
   "metadata": {},
   "source": [
    "## Task 3: Feature Engineering\n",
    "\n",
    "1. **Categorical Variables:**\n",
    "   - Encode categorical variables (e.g., `device_type`, `action`, `content_category`) using one-hot encoding or an appropriate method.\n",
    "\n",
    "2. **Additional Features:**\n",
    "   - Create new features if applicable (e.g., aggregated user behavior metrics or additional time-based segments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a8d9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables\n",
    "data_encoded = pd.get_dummies(data, columns=['device_type', 'action', 'content_category'], drop_first=True)\n",
    "print(\"Columns after one-hot encoding:\")\n",
    "print(data_encoded.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7f070",
   "metadata": {},
   "source": [
    "## Task 4: Model Building\n",
    "\n",
    "1. **Train-Test Split:**\n",
    "   - Split the dataset into training and testing sets.\n",
    "\n",
    "2. **Model Selection:**\n",
    "   - Build at least one classification model (e.g., Logistic Regression, Random Forest, or XGBoost) to predict the likelihood of engagement.\n",
    "   - Train your model on the training set.\n",
    "\n",
    "3. **Evaluation:**\n",
    "   - Evaluate your model using metrics such as accuracy, ROC AUC, precision, recall, and F1 score.\n",
    "   - Provide a confusion matrix and a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3a681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "# Define features (X) and target (y). Drop columns that are identifiers or not needed.\n",
    "X = data_encoded.drop(columns=['engagement', 'session_id', 'user_id', 'timestamp'])\n",
    "y = data_encoded['engagement']\n",
    "\n",
    "# Split the data (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train a Random Forest classifier\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d18692f",
   "metadata": {},
   "source": [
    "## Task 5: Model Tuning and Evaluation\n",
    "\n",
    "1. **Hyperparameter Tuning:**\n",
    "   - Use cross-validation and GridSearchCV (or a similar approach) to optimize your model’s hyperparameters.\n",
    "   - Report the best hyperparameter settings and performance metrics.\n",
    "\n",
    "2. **Feature Importance:**\n",
    "   - Identify and visualize the most important features influencing your model’s predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989a238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a grid of hyperparameters\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), \n",
    "                           param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validated ROC AUC:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "print(\"\\nBest Model Accuracy:\", accuracy_score(y_test, y_pred_best))\n",
    "print(\"Best Model ROC AUC Score:\", roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1]))\n",
    "\n",
    "# Plot feature importances\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "importances = best_model.feature_importances_\n",
    "features = X.columns\n",
    "feature_importance = pd.Series(importances, index=features).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=feature_importance.values, y=feature_importance.index)\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a8d787",
   "metadata": {},
   "source": [
    "## Task 6: ML Ops and Production Readiness\n",
    "\n",
    "Answer the following questions in your submission. Provide written explanations and, where applicable, code snippets or diagrams:\n",
    "\n",
    "1. **Model Deployment:**\n",
    "   - How would you deploy your trained model in a production environment (e.g., using REST APIs, containerization)?\n",
    "   - Which platforms or tools would you use (e.g., AWS SageMaker, Docker, Kubernetes) and why?\n",
    "\n",
    "2. **Monitoring and Logging:**\n",
    "   - Describe how you would monitor the model's performance and detect data drift over time.\n",
    "   - What logging and alerting mechanisms would you implement?\n",
    "\n",
    "3. **CI/CD Pipeline for ML:**\n",
    "   - Outline your strategy for setting up a continuous integration and deployment (CI/CD) pipeline for machine learning models.\n",
    "   - Mention specific tools or practices (e.g., model versioning, automated testing, rollback mechanisms).\n",
    "\n",
    "4. **Scalability and Maintenance:**\n",
    "   - How would you ensure that your model scales to handle high volumes of data and prediction requests?\n",
    "   - Discuss strategies for model retraining and versioning in response to evolving data.\n",
    "\n",
    "*(Please provide your detailed answers as additional markdown cells in your submission.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c8c9e3-fb3c-405a-9cc4-293c3a725f80",
   "metadata": {},
   "source": [
    "# ML Ops and Production Readiness: Answers\n",
    "\n",
    "Below are detailed responses to the ML Ops questions:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Model Deployment\n",
    "\n",
    "**Approach:**\n",
    "- **Containerization:** I would containerize the trained model using Docker to ensure consistency between development and production environments. This allows the model to be packaged with all its dependencies.\n",
    "- **API Serving:** Deploy the container as a REST API using frameworks like Flask or FastAPI. This provides a simple endpoint for real-time predictions.\n",
    "- **Orchestration and Scaling:** For production, I would deploy the containerized model on a managed Kubernetes cluster (e.g., AWS EKS, GCP GKE, or Azure AKS) to handle scaling, load balancing, and fault tolerance.\n",
    "- **Managed Services:** Alternatively, I might leverage a managed service like AWS SageMaker Endpoints for model serving, which abstracts away many operational complexities.\n",
    "\n",
    "**Rationale:**  \n",
    "Using containerization and orchestration tools ensures that the model can scale dynamically with user traffic while maintaining high availability and easy updates.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Monitoring and Logging\n",
    "\n",
    "**Monitoring Strategy:**\n",
    "- **Performance Metrics:** I would monitor key metrics such as request latency, throughput, error rates, and model-specific metrics (e.g., prediction confidence, distribution of input features).\n",
    "- **Drift Detection:** Implement statistical tests or use tools (e.g., Evidently AI) to compare the live data distribution with the training data, thereby detecting data drift.\n",
    "\n",
    "**Logging and Alerting:**\n",
    "- **Centralized Logging:** Use centralized logging systems like the ELK Stack (Elasticsearch, Logstash, Kibana) or cloud-native solutions (e.g., AWS CloudWatch) to capture logs from the model server.\n",
    "- **Alerting Mechanisms:** Set up alerting via Prometheus with Alertmanager or cloud-based alerts that notify the team via email or messaging platforms (e.g., Slack) when anomalies are detected.\n",
    "\n",
    "**Rationale:**  \n",
    "A robust monitoring system is essential to promptly detect and address issues in production, ensuring that the model remains reliable and performs as expected over time.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. CI/CD Pipeline for ML\n",
    "\n",
    "**CI/CD Strategy:**\n",
    "- **Version Control and Automated Testing:** Integrate the model code and training scripts with Git and set up automated testing (e.g., unit tests, integration tests) using GitHub Actions, Jenkins, or GitLab CI/CD.\n",
    "- **Model Versioning:** Use tools such as MLflow or DVC to track experiments, manage model artifacts, and version deployed models.\n",
    "- **Container Build and Deployment:** Automate the process of building Docker images and deploying them to a staging environment using Kubernetes or managed services like AWS SageMaker.\n",
    "- **Rollback Mechanisms:** Implement rollback strategies so that if a new model version underperforms, the system can automatically revert to a previously stable version.\n",
    "\n",
    "**Rationale:**  \n",
    "Automating the CI/CD pipeline minimizes human error, ensures reproducibility, and allows for rapid iteration and deployment of model updates.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Scalability and Maintenance\n",
    "\n",
    "**Scalability:**\n",
    "- **Auto-scaling Infrastructure:** Utilize Kubernetes’ horizontal pod autoscaling to dynamically adjust the number of deployed containers based on the load.\n",
    "- **Load Balancing:** Implement load balancing to distribute prediction requests evenly across multiple instances, ensuring efficient utilization of resources.\n",
    "\n",
    "**Maintenance and Retraining:**\n",
    "- **Scheduled Retraining:** Set up periodic retraining pipelines triggered by data drift alerts or scheduled intervals (e.g., weekly or monthly retraining) to keep the model current.\n",
    "- **Continuous Monitoring:** Regularly monitor performance metrics to identify degradation, which could trigger retraining or model updates.\n",
    "- **Model Versioning:** Maintain a history of model versions to enable A/B testing and facilitate smooth transitions between model updates.\n",
    "\n",
    "**Rationale:**  \n",
    "Ensuring scalability and continuous maintenance of the model is key to handling high volumes of data and evolving user behavior, which is critical in a dynamic production environment like Facebook.\n",
    "\n",
    "---\n",
    "\n",
    "*These answers reflect a comprehensive strategy for deploying, monitoring, and maintaining machine learning models in production, ensuring that the system is robust, scalable, and continuously improved based on live data and performance metrics.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abf96ea",
   "metadata": {},
   "source": [
    "## Submission Instructions\n",
    "\n",
    "- Provide your Python code in this notebook.\n",
    "- Include your answers to the ML Ops questions in markdown cells within this notebook or attach supplementary documentation if needed.\n",
    "- Ensure that your code is well-commented and organized, and that visualizations and outputs are clearly labeled.\n",
    "- Submit your notebook along with any supplementary documentation (e.g., diagrams for CI/CD pipelines) that supports your solutions.\n",
    "\n",
    "*Good luck, and we look forward to reviewing your submission!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
