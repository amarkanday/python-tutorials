{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc6f233-1578-46d3-a9cc-950060ab401a",
   "metadata": {},
   "source": [
    "# Short Python Tutorial: Data Analysis, Machine Learning, and Statistical Techniques\n",
    "\n",
    "This tutorial aims to provide a complete learning experience in Python, focusing on data analysis, machine learning, and statistical methods. Each section includes explanations, code examples, and practical exercises to help you understand and apply these concepts effectively.\n",
    "\n",
    "## 1. Introduction to Python\n",
    "\n",
    "Python is a versatile, high-level programming language known for its readability and ease of use. It is commonly used for data analysis, web development, automation, and machine learning.\n",
    "\n",
    "- **Key Features**: Interpreted, object-oriented, dynamically typed.\n",
    "- **Installation**: Use [Python.org](https://www.python.org/) or install via Anaconda, a data science platform.\n",
    "- **Tools**: Jupyter Notebook for interactive coding, VS Code or PyCharm for more sophisticated IDE features.\n",
    "\n",
    "## 2. Data Types in Python\n",
    "\n",
    "Python supports several data types:\n",
    "\n",
    "- **Numeric**: int, float, complex.\n",
    "- **Sequence**: list, tuple, range.\n",
    "- **Text**: str.\n",
    "- **Boolean**: bool.\n",
    "- **NoneType**: None.\n",
    "\n",
    "**Example Code**:\n",
    "\n",
    "```python\n",
    "x = 10          # int\n",
    "y = 3.14        # float\n",
    "name = \"Alice\"  # str\n",
    "is_valid = True # bool\n",
    "```\n",
    "\n",
    "**Exercise**: Define variables of each data type, print their values, and determine their type using `type()`.\n",
    "\n",
    "## 3. Data Structures\n",
    "\n",
    "Python has powerful built-in data structures:\n",
    "\n",
    "- **Lists**: Ordered, mutable collections.\n",
    "- **Tuples**: Ordered, immutable collections.\n",
    "- **Sets**: Unordered, unique collections.\n",
    "- **Dictionaries**: Key-value pairs.\n",
    "\n",
    "**Example Code**:\n",
    "\n",
    "```python\n",
    "fruits = [\"apple\", \"banana\", \"cherry\"]  # List\n",
    "coordinates = (10.5, 20.7)                  # Tuple\n",
    "unique_numbers = {1, 2, 3, 3}               # Set\n",
    "data = {\"name\": \"Alice\", \"age\": 30}    # Dictionary\n",
    "```\n",
    "\n",
    "**Exercise**: Create and modify each data structure. Practice adding and removing items.\n",
    "\n",
    "## 4. Datetime in Python\n",
    "\n",
    "Python's `datetime` module lets you work with dates and times.\n",
    "\n",
    "- **Creating DateTime objects**:\n",
    "```python\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "print(now)\n",
    "```\n",
    "- **Extracting Components**:\n",
    "```python\n",
    "print(now.year, now.month, now.day)\n",
    "```\n",
    "\n",
    "**Exercise**: Create a function that takes a date string and converts it to a `datetime` object.\n",
    "\n",
    "## 5. Introduction to Pandas, NumPy, and Scikit-Learn\n",
    "Pandas, NumPy, and Scikit-Learn are powerful Python libraries for data manipulation, analysis, and machine learning. Pandas provides data structures such as Series and DataFrame that make handling data intuitive, while NumPy is useful for performing fast mathematical operations on arrays. Scikit-Learn is a popular library for machine learning, providing various algorithms and tools for model training and evaluation. In this notebook, I'll walk you through some basic operations and functionalities in Pandas, NumPy, and Scikit-Learn with examples.\n",
    "\n",
    "```python\n",
    "# Importing pandas, numpy, seaborn, and scikit-learn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report\n",
    "```\n",
    "\n",
    "### 5.1 Key Data Structures\n",
    "\n",
    "### Series (Pandas)\n",
    "A **Series** is a one-dimensional array-like object that can hold data of any type. It's similar to a column in an Excel spreadsheet.\n",
    "\n",
    "```python\n",
    "# Creating a Series\n",
    "data = [10, 20, 30, 40, 50]\n",
    "series = pd.Series(data)\n",
    "\n",
    "# Display the Series\n",
    "print(series)\n",
    "```\n",
    "\n",
    "A **Series** can also have custom indices:\n",
    "\n",
    "```python\n",
    "# Series with custom index\n",
    "series_custom_index = pd.Series(data, index=['a', 'b', 'c', 'd', 'e'])\n",
    "print(series_custom_index)\n",
    "```\n",
    "\n",
    "### NumPy Array\n",
    "A **NumPy array** is a multi-dimensional array object that provides fast and efficient operations on data.\n",
    "\n",
    "```python\n",
    "# Creating a NumPy array\n",
    "array = np.array([1, 2, 3, 4, 5])\n",
    "print(array)\n",
    "```\n",
    "\n",
    "NumPy arrays support vectorized operations, which makes mathematical operations much faster.\n",
    "\n",
    "```python\n",
    "# Vectorized operations\n",
    "array_squared = array ** 2\n",
    "print(array_squared)\n",
    "```\n",
    "\n",
    "### 5.2 DataFrame (Pandas)\n",
    "A **DataFrame** is a two-dimensional data structure that represents data in a table, similar to an Excel spreadsheet or SQL table.\n",
    "\n",
    "```python\n",
    "# Creating a DataFrame from a dictionary\n",
    "data = {\n",
    "'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "'Age': [25, 30, 35],\n",
    "'City': ['New York', 'San Francisco', 'Los Angeles']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "```\n",
    "\n",
    "### 5.3 Data Loading and Viewing (Pandas)\n",
    "Pandas can load data from various sources, such as CSV files, Excel sheets, databases, etc.\n",
    "\n",
    "```python\n",
    "# Loading a CSV file\n",
    "df = pd.read_csv('example.csv')  # Replace 'example.csv' with your file name\n",
    "\n",
    "# Viewing data\n",
    "print(df.head())  # Displays the first 5 rows by default\n",
    "print(df.tail(3))  # Displays the last 3 rows\n",
    "print(df.info())  # Displays concise summary of the DataFrame\n",
    "print(df.describe())  # Statistical summary for numerical columns\n",
    "```\n",
    "\n",
    "### 5.4 Data Selection and Filtering (Pandas)\n",
    "\n",
    "### Selecting Columns and Rows\n",
    "- **Selecting a Column**:\n",
    "\n",
    "```python\n",
    "age_column = df['Age']\n",
    "print(age_column)\n",
    "```\n",
    "\n",
    "- **Selecting Multiple Columns**:\n",
    "\n",
    "```python\n",
    "subset = df[['Name', 'City']]\n",
    "print(subset)\n",
    "```\n",
    "\n",
    "- **Selecting Rows using `.loc[]`**:\n",
    "\n",
    "```python\n",
    "# Selecting row by index label\n",
    "print(df.loc[1])\n",
    "```\n",
    "\n",
    "- **Selecting Rows using `.iloc[]`**:\n",
    "\n",
    "```python\n",
    "# Selecting row by index position\n",
    "print(df.iloc[0:2])  # Selects the first two rows\n",
    "```\n",
    "\n",
    "### 5.5 Conditional Selection\n",
    "\n",
    "```python\n",
    "# Filtering rows based on condition\n",
    "filtered_df = df[df['Age'] > 28]\n",
    "print(filtered_df)\n",
    "```\n",
    "\n",
    "### 5.6 Data Manipulation\n",
    "\n",
    "### Handling Missing Values (Pandas)\n",
    "\n",
    "- **Check for Missing Values**:\n",
    "\n",
    "```python\n",
    "print(df.isnull().sum())  # Counts missing values in each column\n",
    "```\n",
    "\n",
    "- **Fill Missing Values**:\n",
    "\n",
    "```python\n",
    "df['Age'].fillna(df['Age'].mean(), inplace=True)  # Fills NaNs with mean value of the 'Age' column\n",
    "```\n",
    "\n",
    "- **Drop Missing Values**:\n",
    "\n",
    "```python\n",
    "df.dropna(inplace=True)  # Drops rows with any missing values\n",
    "```\n",
    "\n",
    "### 5.7 Mathematical Operations with NumPy\n",
    "\n",
    "- **Basic Operations**:\n",
    "\n",
    "```python\n",
    "array = np.array([10, 20, 30, 40, 50])\n",
    "print(np.sum(array))  # Sum of all elements\n",
    "print(np.mean(array))  # Mean of the array\n",
    "print(np.max(array))  # Maximum value in the array\n",
    "```\n",
    "\n",
    "- **Reshaping Arrays**:\n",
    "\n",
    "```python\n",
    "reshaped_array = array.reshape(5, 1)  # Reshape to 5 rows, 1 column\n",
    "print(reshaped_array)\n",
    "```\n",
    "\n",
    "### 5.8 Grouping and Aggregation (Pandas)\n",
    "\n",
    "- **Grouping Data**:\n",
    "\n",
    "```python\n",
    "grouped_df = df.groupby('City').mean()  # Group by 'City' and calculate mean for numerical columns\n",
    "print(grouped_df)\n",
    "```\n",
    "\n",
    "- **Aggregating Data**:\n",
    "\n",
    "```python\n",
    "aggregated = df.groupby('City').agg({'Age': ['mean', 'max']})\n",
    "print(aggregated)\n",
    "```\n",
    "\n",
    "### 5.9 Merging and Joining (Pandas)\n",
    "- **Merging DataFrames**:\n",
    "\n",
    "```python\n",
    "df1 = pd.DataFrame({'Name': ['Alice', 'Bob'], 'Age': [25, 30]})\n",
    "df2 = pd.DataFrame({'Name': ['Alice', 'Bob'], 'City': ['New York', 'San Francisco']})\n",
    "\n",
    "merged_df = pd.merge(df1, df2, on='Name')\n",
    "print(merged_df)\n",
    "```\n",
    "\n",
    "- **Concatenating DataFrames**:\n",
    "\n",
    "```python\n",
    "df1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
    "df2 = pd.DataFrame({'A': [5, 6], 'B': [7, 8]})\n",
    "\n",
    "concatenated_df = pd.concat([df1, df2])\n",
    "print(concatenated_df)\n",
    "```\n",
    "\n",
    "**Exercise**: Load a dataset, identify null values, and fill them with different strategies.\n",
    "\n",
    "### 5.10 Handling Outliers\n",
    "\n",
    "- **Using IQR**:\n",
    "```python\n",
    "Q1 = df[\"column\"].quantile(0.25)\n",
    "Q3 = df[\"column\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = df[(df[\"column\"] < (Q1 - 1.5 * IQR)) | (df[\"column\"] > (Q3 + 1.5 * IQR))]\n",
    "```\n",
    "\n",
    "**Exercise**: Detect and handle outliers in a sample dataset.\n",
    "\n",
    "- **Visualization with Seaborn**:\n",
    "\n",
    "```python\n",
    "# Scatter Plot with Seaborn\n",
    "sns.scatterplot(data=df, x='Age', y='Name')\n",
    "plt.title('Age vs Name')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Name')\n",
    "plt.show()\n",
    "\n",
    "# Box Plot with Seaborn\n",
    "sns.boxplot(data=df, x='City', y='Age')\n",
    "plt.title('Age Distribution by City')\n",
    "plt.xlabel('City')\n",
    "plt.ylabel('Age')\n",
    "plt.show()\n",
    "\n",
    "# Heatmap with Seaborn (Correlation)\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "**Exercise**: Use seaborn to visualize relationships in the `tips` dataset.\n",
    "\n",
    "## 6. Data Scaling, Encoding & Feature Engineering\n",
    "\n",
    "- **Scaling**:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "```\n",
    "\n",
    "- **Dummy Variables**:\n",
    "\n",
    "```python\n",
    "df_dummies = pd.get_dummies(df, columns=[\"category_column\"])\n",
    "```\n",
    "\n",
    "**Exercise**: Scale and encode features in a sample dataset.\n",
    "\n",
    
    "## 7. Supervised Learning\n",
    "\n",
    "### Overview\n",
    "In **supervised learning**, you train a model using labeled data, which means that each training example has a corresponding output. The model learns the relationship between input and output, and you can use it to predict the output for new, unseen data. Supervised learning is divided into two types:\n",
    "\n",
    "- **Classification**: Predict categorical outputs (e.g., classifying an email as spam or not spam).\n",
    "- **Regression**: Predict continuous outputs (e.g., predicting house prices).\n",
    "\n",
    "### Example 7.1: Linear Regression with Python\n",
    "\n",
    "**Linear Regression** is a simple type of regression where the relationship between the input (independent) and output (dependent) variable is represented as a line. Below is an example using Python's popular machine learning library `scikit-learn`.\n",
    "\n",
    "#### Step-by-Step Code for Linear Regression\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generating synthetic dataset\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating a Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R² Score:\", r2_score(y_test, y_pred))\n",
    "\n",
    "# Visualizing the results\n",
    "plt.scatter(X, y, color='blue', label='Data Points')\n",
    "plt.plot(X, model.predict(X), color='red', linewidth=2, label='Regression Line')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Explanation\n",
    "1. **Data Generation**: We generated random data that fits the equation `y = 4 + 3*X + noise`.\n",
    "2. **Training the Model**: We use `train_test_split` to divide the data into training and testing sets.\n",
    "3. **Model Training**: `LinearRegression()` is used to create the model, and `.fit()` trains it on the training data.\n",
    "4. **Prediction and Evaluation**: We evaluate using `mean_squared_error` and `r2_score`.\n",
    "\n",
    "This example demonstrates how you can predict continuous values, like house prices, based on one or more features.\n",
    "\n",
    "### Example 7.2: Classification with Python (K-Nearest Neighbors)\n",
    "\n",
    "A **Classification Problem** can be solved using algorithms like K-Nearest Neighbors (KNN). Here, we'll classify whether a given iris flower belongs to a specific species based on its characteristics.\n",
    "\n",
    "#### Step-by-Step Code for Classification Using KNN\n",
    "\n",
    "```python\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Loading Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Creating and training the KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "```\n",
    "\n",
    "#### Explanation\n",
    "1. **Dataset**: The Iris dataset is a standard dataset for classification, containing flower measurements for three species.\n",
    "2. **Training the Model**: A KNN model is trained with `k=3`, which means we use the 3 nearest neighbors to classify a point.\n",
    "3. **Prediction and Evaluation**: The model’s accuracy is measured against the test set.\n",
    "\n",
    "### Supervised Learning: XGBoost Example and Hyperparameter Tuning\n",
    "\n",
    "### 7.3 XGBoost for Classification\n",
    "Let's look at an example where we use **XGBoost** to classify the Iris dataset.\n",
    "\n",
    "#### Step-by-Step Code for XGBoost Classification\n",
    "\n",
    "```python\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Loading the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Creating the XGBoost DMatrix\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Setting parameters for the XGBoost model\n",
    "params = {\n",
    "'objective': 'multi:softmax',  # For classification\n",
    "'num_class': 3,  # Number of classes in the target\n",
    "'max_depth': 3,  # Maximum depth of a tree\n",
    "'learning_rate': 0.1,  # Learning rate\n",
    "'eval_metric': 'mlogloss',  # Evaluation metric\n",
    "'seed': 42  # Random seed\n",
    "}\n",
    "\n",
    "# Training the XGBoost model\n",
    "bst = xgb.train(params, dtrain, num_boost_round=50)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = bst.predict(dtest)\n",
    "\n",
    "# Evaluating the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of XGBoost model:\", accuracy)\n",
    "```\n",
    "\n",
    "#### Explanation\n",
    "1. **Dataset**: The Iris dataset is loaded, and `X` and `y` are split into training and test sets.\n",
    "2. **DMatrix**: XGBoost uses `DMatrix` to store data efficiently.\n",
    "3. **Parameter Setting**: Parameters are set such as `objective`, `num_class`, `max_depth`, and `learning_rate`.\n",
    "4. **Training**: The model is trained using `.train()`.\n",
    "5. **Prediction and Evaluation**: We evaluate using `accuracy_score`.\n",
    "\n",
    "### 7.4 Hyperparameter Tuning using GridSearchCV\n",
    "\n",
    "Hyperparameter tuning is crucial to improve model performance, and in Python, we can use `GridSearchCV` from `scikit-learn` to tune the parameters for the XGBoost model.\n",
    "\n",
    "Below, we perform hyperparameter tuning for XGBoost using **GridSearchCV**.\n",
    "\n",
    "#### Step-by-Step Code for Hyperparameter Tuning\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "# Splitting the Iris dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Creating the XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=3, seed=42)\n",
    "\n",
    "# Defining the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "'max_depth': [3, 5, 7],\n",
    "'learning_rate': [0.01, 0.1, 0.2],\n",
    "'n_estimators': [50, 100, 150],\n",
    "'gamma': [0, 0.1, 0.2],\n",
    "'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Performing GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Getting the best parameters from grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters from GridSearch:\", best_params)\n",
    "\n",
    "# Evaluating the model with the best parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy after hyperparameter tuning:\", accuracy)\n",
    "```\n",
    "\n",
    "#### Explanation\n",
    "1. **Parameter Grid**: We define the `param_grid` with multiple values for `max_depth`, `learning_rate`, `n_estimators`, `gamma`, and `subsample`.\n",
    "2. **GridSearchCV**: We use `GridSearchCV` to find the best combination of hyperparameters. Here, `cv=3` means we are using 3-fold cross-validation.\n",
    "3. **Best Model**: The `best_estimator_` from the `grid_search` object is the model with the best parameters.\n",
    "4. **Evaluation**: We predict and evaluate the performance using `accuracy_score`.\n",
    "\n",
    "### Hyperparameters for XGBoost\n",
    "\n",
    "Below are some of the important hyperparameters we tuned:\n",
    "\n",
    "1. **`max_depth`**: The maximum depth of a tree. Higher values might lead to overfitting.\n",
    "2. **`learning_rate`**: The step size to update weights in each iteration. Lower values make the model learn slowly.\n",
    "3. **`n_estimators`**: The number of boosting rounds.\n",
    "4. **`gamma`**: A regularization parameter. It controls whether a given leaf will split based on its gain.\n",
    "5. **`subsample`**: The proportion of training data that is randomly selected for each boosting round.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **XGBoost**: A high-performance, efficient gradient boosting framework, popular for solving both classification and regression tasks.\n",
    "- **Hyperparameter Tuning**: The process of finding the best set of hyperparameters to improve model performance.\n",
    "- We used **GridSearchCV** to perform a systematic search over hyperparameter combinations.\n",
    "\n",
    "In practice, hyperparameter tuning for **XGBoost** can significantly improve your model’s accuracy and reduce overfitting. You could also use other techniques like **RandomizedSearchCV** or Bayesian optimization to search for optimal hyperparameters efficiently.\n",
    "\n",
    "By combining a powerful model like **XGBoost** with robust hyperparameter tuning, you can often achieve state-of-the-art results on many machine learning tasks.\n",
    "\n",
    "\n",
    "## 8. Unsupervised Learning\n",
    "\n",
    "### Overview\n",
    "In **unsupervised learning**, the model is trained using **unlabeled data**. Here, the model identifies hidden patterns or groupings without knowing the specific labels or outputs. The two main types of unsupervised learning are:\n",
    "\n",
    "- **Clustering**: Grouping similar data points together (e.g., customer segmentation).\n",
    "- **Dimensionality Reduction**: Reducing the number of features while retaining most of the information.\n",
    "\n",
    "### Example 8.1: Clustering with K-Means\n",
    "\n",
    "A common unsupervised learning algorithm is **K-Means Clustering**, which groups data points into `k` clusters based on their features.\n",
    "\n",
    "#### Step-by-Step Code for K-Means Clustering\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generating synthetic dataset for clustering\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.6, random_state=0)\n",
    "\n",
    "# Creating and fitting the K-Means model\n",
    "kmeans = KMeans(n_clusters=4, random_state=0)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "# Plotting the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis', s=50)\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, label='Cluster Centers')\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Explanation\n",
    "1. **Data Generation**: We generate synthetic data using `make_blobs` that can easily be clustered.\n",
    "2. **Training the Model**: The `KMeans` model with `n_clusters=4` finds clusters in the data.\n",
    "3. **Plotting the Results**: The clusters are visualized, and the cluster centers are shown in red.\n",
    "\n",
    "### Example 8.2: Principal Component Analysis (PCA)\n",
    "\n",
    "**Principal Component Analysis (PCA)** is a dimensionality reduction technique used to reduce the number of features while retaining important information. It’s often used for visualization.\n",
    "\n",
    "#### Step-by-Step Code for PCA\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Applying PCA to reduce to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plotting the reduced dimensions\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', s=50)\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"PCA of Iris Dataset\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Explanation\n",
    "1. **Dataset**: The Iris dataset is loaded.\n",
    "2. **Dimensionality Reduction**: PCA is used to reduce the 4-dimensional data to 2 dimensions.\n",
    "3. **Visualization**: The resulting 2D representation is plotted, which helps us understand how similar the different iris species are in a reduced space.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Supervised Learning** uses labeled data to train models for predicting known outputs. Examples include **Linear Regression** and **KNN Classification**.\n",
    "- **Unsupervised Learning** identifies patterns without labeled data. Examples include **K-Means Clustering** and **PCA** for dimensionality reduction.\n",
    "\n",
    "Certainly! Let's add an example of **XGBoost** and how to perform **Hyperparameter Tuning** for better model performance. **XGBoost** (Extreme Gradient Boosting) is a powerful and efficient implementation of the Gradient Boosting framework, and it's widely used for both **classification** and **regression** problems due to its speed and performance.\n",
    "\n",
    "## 9. Time Series Analysis\n",
    "\n",
    "Time series analysis involves studying data points collected or recorded at specific time intervals. It is crucial in fields like finance, economics, and environmental studies to identify trends, seasonal patterns, and forecasting.\n",
    "\n",
    "### 9.1 Time Series Decomposition\n",
    "\n",
    "- **Decomposition**: Decomposing a time series into trend, seasonality, and residual components helps us understand the underlying patterns.\n",
    "```python\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "result = seasonal_decompose(time_series, model='additive')\n",
    "result.plot()\n",
    "```\n",
    "\n",
    "**Exercise**: Analyze a time series dataset and decompose it to observe trends and seasonality.\n",
    "\n",
    "### 9.2 Time Series Forecasting\n",
    "\n",
    "- **Autoregressive Integrated Moving Average (ARIMA)**: ARIMA is a popular model for forecasting time series data. It combines autoregressive (AR), differencing (I), and moving average (MA) components.\n",
    "```python\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "model = ARIMA(time_series, order=(1, 1, 1))\n",
    "model_fit = model.fit()\n",
    "print(model_fit.summary())\n",
    "model_fit.plot_predict(dynamic=False)\n",
    "```\n",
    "\n",
    "**Exercise**: Use ARIMA to forecast future values in a given time series dataset.\n",
    "\n",
    "### 9.3 Stationarity and Differencing\n",
    "\n",
    "- **Stationarity**: A time series is said to be stationary if its statistical properties do not change over time. Checking stationarity is important for ARIMA models.\n",
    "- **Dickey-Fuller Test**: Used to test the stationarity of a time series.\n",
    "```python\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "result = adfuller(time_series)\n",
    "print(f'ADF Statistic: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "```\n",
    "\n",
    "- **Differencing**: Used to make a time series stationary by removing trends.\n",
    "```python\n",
    "time_series_diff = time_series.diff().dropna()\n",
    "```\n",
    "\n",
    "**Exercise**: Apply the Dickey-Fuller test on a time series dataset and use differencing to make it stationary.\n",
    "\n",
    "### 9.4 Autocorrelation and Partial Autocorrelation\n",
    "\n",
    "- **ACF and PACF**: Used to identify the order of AR and MA components in a time series model.\n",
    "```python\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "plot_acf(time_series)\n",
    "plot_pacf(time_series)\n",
    "```\n",
    "\n",
    "**Exercise**: Plot the ACF and PACF for a sample time series and interpret the plots.\n",
    "\n",
    "### 9.5 Seasonal ARIMA (SARIMA)\n",
    "\n",
    "- **SARIMA**: A variant of ARIMA that also accounts for seasonality in the time series data.\n",
    "```python\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "model = SARIMAX(time_series, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
    "model_fit = model.fit()\n",
    "model_fit.plot_diagnostics()\n",
    "```\n",
    "\n",
    "**Exercise**: Fit a SARIMA model to a time series dataset with seasonal components.\n",
    "\n",
    "### 9.6 Model Evaluation for Time Series\n",
    "\n",
    "- **Mean Absolute Error (MAE), Root Mean Squared Error (RMSE)**: Common metrics for evaluating the accuracy of time series models.\n",
    "```python\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "predictions = model_fit.forecast(steps=10)\n",
    "mae = mean_absolute_error(test_data, predictions)\n",
    "rmse = mean_squared_error(test_data, predictions, squared=False)\n",
    "print(f'MAE: {mae}, RMSE: {rmse}')\n",
    "```\n",
    "\n",
    "**Exercise**: Evaluate the performance of your ARIMA or SARIMA model using MAE and RMSE metrics.\n",
    "\n",
    "## 10. Recommendation Systems\n",
    "\n",
    "Recommendation systems are algorithms that provide users with personalized suggestions. They can be broadly classified into two types: **Collaborative Filtering** and **Content-Based Filtering**.\n",
    "\n",
    "### 10.1 Collaborative Filtering\n",
    "Collaborative Filtering relies on the assumption that users who have agreed in the past will agree in the future. It uses historical interactions (e.g., user ratings) to predict the preferences of a user based on similar users.\n",
    "\n",
    "- **User-Based Collaborative Filtering**: Find similar users and recommend items liked by those users.\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# User-Item Matrix\n",
    "user_item_matrix = pd.DataFrame({\n",
    "'Item1': [5, 3, 0, 0],\n",
    "'Item2': [4, 0, 0, 0],\n",
    "'Item3': [0, 0, 4, 0],\n",
    "'Item4': [0, 0, 5, 3]\n",
    "}, index=['User1', 'User2', 'User3', 'User4'])\n",
    "\n",
    "# Calculate Similarity\n",
    "similarity = cosine_similarity(user_item_matrix)\n",
    "similarity_df = pd.DataFrame(similarity, index=user_item_matrix.index, columns=user_item_matrix.index)\n",
    "print(similarity_df)\n",
    "```\n",
    "\n",
    "- **Item-Based Collaborative Filtering**: Find similar items and recommend them to users who have interacted with similar items.\n",
    "```python\n",
    "# Calculate Similarity Between Items\n",
    "item_similarity = cosine_similarity(user_item_matrix.T)\n",
    "item_similarity_df = pd.DataFrame(item_similarity, index=user_item_matrix.columns, columns=user_item_matrix.columns)\n",
    "print(item_similarity_df)\n",
    "```\n",
    "\n",
    "**Exercise**: Implement user-based and item-based collaborative filtering using the `MovieLens` dataset and make movie recommendations for a given user.\n",
    "\n",
    "### 10.2 Content-Based Filtering\n",
    "Content-Based Filtering recommends items that are similar to those a user has previously shown interest in. It uses the item's attributes rather than relying on user interaction history.\n",
    "\n",
    "- **TF-IDF for Text Similarity**: Use the `TfidfVectorizer` to recommend items similar to user preferences.\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Sample dataset with movie descriptions\n",
    "data = {\n",
    "'Title': ['Movie1', 'Movie2', 'Movie3'],\n",
    "'Description': ['Action movie with heroes', 'Romantic drama', 'Sci-fi thriller']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Compute TF-IDF\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df['Description'])\n",
    "\n",
    "# Compute Similarity\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "similarity_df = pd.DataFrame(cosine_sim, index=df['Title'], columns=df['Title'])\n",
    "print(similarity_df)\n",
    "```\n",
    "\n",
    "**Exercise**: Build a content-based recommendation system using TF-IDF for the movie dataset and recommend similar movies for a given one.\n",
    "\n",
    "**Comparison of Collaborative and Content-Based Filtering**:\n",
    "- **Collaborative Filtering** relies on past interactions and works well for diverse users but may face the cold start problem (new users/items).\n",
    "- **Content-Based Filtering** uses item features and is effective for recommending similar content but may lack novelty (i.e., recommendations are often too similar).\n",
    "\n",
    "**Exercise**: Compare both methods by implementing them on the same dataset and evaluate which provides better recommendations.\n",
    "\n",
    "\n",
    "**Exercise**: Implement a basic collaborative filtering recommendation using the `MovieLens` dataset.\n",
    "\n",
    "## 11. Statistical Analysis and A/B Testing\n",
    "\n",
    "A/B testing is a statistical method used to compare two versions of a product to determine which one performs better. It is commonly used in marketing, web development, and UX design to evaluate changes to a webpage or product feature.\n",
    "\n",
    "### 11.1 Statistical Analysis\n",
    "\n",
    "- **T-Test**: Used to compare the means of two groups to determine if they are statistically different from each other.\n",
    "```python\n",
    "from scipy.stats import ttest_ind\n",
    "t_stat, p_value = ttest_ind(group1, group2)\n",
    "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
    "```\n",
    "- **Interpretation**: If the p-value is less than a chosen significance level (e.g., 0.05), we reject the null hypothesis and conclude that there is a significant difference between the groups.\n",
    "\n",
    "- **ANOVA (Analysis of Variance)**: Used to compare the means of three or more groups.\n",
    "```python\n",
    "from scipy.stats import f_oneway\n",
    "f_stat, p_value = f_oneway(group1, group2, group3)\n",
    "print(f\"F-statistic: {f_stat}, P-value: {p_value}\")\n",
    "```\n",
    "- **Interpretation**: Similar to the t-test, if the p-value is below the significance level, we conclude that at least one group is significantly different.\n",
    "\n",
    "- **Chi-Square Test**: Used for categorical data to determine if there is an association between two variables.\n",
    "```python\n",
    "from scipy.stats import chi2_contingency\n",
    "contingency_table = [[10, 20, 30], [6, 9, 17]]\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "print(f\"Chi2: {chi2}, P-value: {p}\")\n",
    "```\n",
    "- **Interpretation**: A low p-value indicates a significant association between the categorical variables.\n",
    "\n",
    "**Exercise**: Perform t-tests, ANOVA, and Chi-Square tests on sample datasets to practice statistical analysis.\n",
    "\n",
    "### 11.2 A/B Testing\n",
    "\n",
    "A/B testing is used to compare two versions of a webpage, feature, or treatment to determine which performs better. Typically, metrics like click-through rate (CTR), conversion rate, or engagement are compared between the two groups (A and B).\n",
    "\n",
    "- **Steps in A/B Testing**:\n",
    "1. **Define Hypotheses**: Define the null and alternative hypotheses.\n",
    "2. **Random Assignment**: Split users randomly into two groups (A and B).\n",
    "3. **Run Experiment**: Expose the groups to different versions.\n",
    "4. **Collect Data**: Gather data on the desired metric.\n",
    "5. **Analyze Results**: Use statistical tests to determine if the difference is significant.\n",
    "\n",
    "- **Example Code for A/B Testing**:\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Simulated data for two groups\n",
    "group_A = np.random.binomial(1, 0.4, 1000)  # 40% conversion rate\n",
    "group_B = np.random.binomial(1, 0.45, 1000) # 45% conversion rate\n",
    "\n",
    "# T-Test to compare means\n",
    "t_stat, p_value = ttest_ind(group_A, group_B)\n",
    "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "print(\"Reject the null hypothesis: There is a significant difference between Group A and Group B.\")\n",
    "else:\n",
    "print(\"Fail to reject the null hypothesis: No significant difference found.\")\n",
    "```\n",
    "\n",
    "- **Types of A/B Tests**:\n",
    "- **Standard A/B Testing**: Compare one control (A) against one variation (B).\n",
    "- **Multivariate Testing**: Test multiple variables and their combinations simultaneously to see which combination performs best.\n",
    "- **Split URL Testing**: Compare completely different landing pages by splitting traffic between different URLs.\n",
    "\n",
    "**Exercise**: Design and implement an A/B test for a webpage feature, simulate the data, and analyze the results using statistical methods.\n",
    "\n",
    "## 12. Feature Importance Using Shapley Values\n",
    "\n",
    "- **SHAP Library**:\n",
    "```python\n",
    "import shap\n",
    "explainer = shap.Explainer(model, X)\n",
    "shap_values = explainer(X)\n",
    "shap.summary_plot(shap_values, X)\n",
    "```\n",
    "\n",
    "**Exercise**: Use SHAP to explain feature importance in a Random Forest model.\n",
    "\n",
    "## 13. Dealing with Dataset Imbalance\n",
    "\n",
    "- **Oversampling Using SMOTE**:\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "```\n",
    "\n",
    "**Exercise**: Apply SMOTE to a dataset with class imbalance and compare model performance.\n",
    "\n",
    " \n",
    "\n",
    "## 14.1 Deep Learning Tutorial: Keras, PyTorch, and Unsupervised Learning in Python\n",
    "\n",
    "Deep learning frameworks like Keras and PyTorch make it easier to build and experiment with deep neural networks. In this tutorial, we'll explore the basics of creating and training a neural network using both Keras (a high-level API in TensorFlow) and PyTorch, two of the most popular deep learning libraries. We will also cover unsupervised learning techniques using Python.\n",
    "\n",
    "### 14.1 Prerequisites\n",
    "To follow along, you should have:\n",
    "- Basic knowledge of Python programming.\n",
    "- An understanding of neural networks.\n",
    "- Installed TensorFlow (with Keras), PyTorch, and scikit-learn. You can install them via pip:\n",
    "  ```sh\n",
    "  pip install tensorflow\n",
    "  pip install torch\n",
    "  pip install scikit-learn\n",
    "  ```\n",
    "\n",
    "### 14.2 Overview\n",
    "In this tutorial, we will implement a simple feedforward neural network to classify digits from the MNIST dataset using both Keras and PyTorch. Additionally, we'll demonstrate an unsupervised learning example using scikit-learn.\n",
    "\n",
    "### 14.3 Part 1: Using Keras\n",
    "Keras is a user-friendly deep learning API, running on top of TensorFlow. Let's start by building and training a neural network with Keras.\n",
    "\n",
    "### Step 1: Load the Dataset\n",
    "Keras has a convenient way to load the MNIST dataset directly from its datasets module.\n",
    "```python\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load data\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Preprocess data\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "```\n",
    "\n",
    "### Step 2: Build the Model\n",
    "We will create a simple feedforward neural network with a few layers using Keras.\n",
    "```python\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "```\n",
    "\n",
    "### 14.4 Step 3: Compile and Train the Model\n",
    "Next, we compile the model and train it on the dataset.\n",
    "```python\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.2)\n",
    "```\n",
    "\n",
    "### 14.5 Step 4: Evaluate the Model\n",
    "To evaluate the model on the test set, run:\n",
    "```python\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy: {test_acc:.2f}')\n",
    "```\n",
    "\n",
    "### 14.6 Part 2: Using PyTorch\n",
    "PyTorch offers more flexibility and is often favored for research purposes. Let's now implement a similar model using PyTorch.\n",
    "\n",
    "### Step 1: Load the Dataset\n",
    "We'll use `torchvision` to load the MNIST dataset.\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transformations and load dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "```\n",
    "\n",
    "### Step 2: Define the Model\n",
    "We define a simple feedforward neural network with convolutional layers using PyTorch `nn.Module`.\n",
    "```python\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 64 * 5 * 5)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "model = SimpleCNN()\n",
    "```\n",
    "\n",
    "### Step 3: Train the Model\n",
    "Define the loss function and optimizer, and train the model.\n",
    "```python\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}')\n",
    "\n",
    "train(model, train_loader, criterion, optimizer)\n",
    "```\n",
    "\n",
    "### Step 4: Evaluate the Model\n",
    "Evaluate the model on the test dataset.\n",
    "```python\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "evaluate(model, test_loader)\n",
    "```\n",
    "\n",
    "### 14.6 Part 3: Unsupervised Learning in Python\n",
    "Unsupervised learning is useful for finding patterns in data without predefined labels. Here, we will use the popular `scikit-learn` library to perform K-Means clustering on the MNIST dataset.\n",
    "\n",
    "### Step 1: Load the Dataset\n",
    "We will load the MNIST dataset using scikit-learn and preprocess it for clustering.\n",
    "```python\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "data = mnist.data / 255.0\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "```\n",
    "\n",
    "### Step 2: Apply K-Means Clustering\n",
    "We will apply K-Means clustering to group the images into clusters.\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "kmeans.fit(data_scaled)\n",
    "\n",
    "# Get the cluster labels\n",
    "cluster_labels = kmeans.labels_\n",
    "```\n",
    "\n",
    "### Step 3: Evaluate Clustering Performance\n",
    "To evaluate clustering performance, we can use metrics like the silhouette score.\n",
    "```python\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Calculate silhouette score\n",
    "score = silhouette_score(data_scaled, cluster_labels)\n",
    "print(f'Silhouette Score: {score:.2f}')\n",
    "```\n",
    "\n",
    "### Conclusion for Deep Learning tutorial \n",
    "In this tutorial, we have implemented a simple deep learning model for digit classification using both Keras and PyTorch, as well as an unsupervised learning example using scikit-learn. Keras provides a high-level interface that makes model building easy and quick, whereas PyTorch offers more control and flexibility, which is great for experimentation. Scikit-learn, on the other hand, is ideal for unsupervised learning tasks like clustering. Your choice of framework will depend on your project's requirements and objectives.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aad460-8f05-49e1-b3bf-e3f161afbd91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
