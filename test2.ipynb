{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abffeaee-33ad-464d-a272-81f8a92ac817",
   "metadata": {},
   "outputs": [],
   "source": [
    "Here is a Python test that assesses advanced data analysis and modeling skills. This test will involve a combination of tasks: data manipulation, exploratory data analysis (EDA), feature engineering, and machine learning modeling. The test focuses on applying various techniques to a real-world dataset, which can be replaced with any dataset of choice.\n",
    "\n",
    "### Task 1: Data Manipulation and Exploration\n",
    "Given a dataset (e.g., Titanic dataset or any other publicly available dataset), the candidate should perform the following tasks:\n",
    "\n",
    "1. **Data Loading and Initial Exploration**:\n",
    "    - Load the dataset and provide a summary of the data.\n",
    "    - Check for missing values and outliers.\n",
    "    - Provide basic statistics and visualizations for numeric and categorical variables.\n",
    "\n",
    "2. **Data Cleaning**:\n",
    "    - Handle missing values (impute or remove).\n",
    "    - Handle outliers using appropriate techniques.\n",
    "\n",
    "3. **Feature Engineering**:\n",
    "    - Create new features based on domain knowledge or interactions between existing variables.\n",
    "    - Encode categorical variables.\n",
    "    - Scale/normalize numerical features if necessary.\n",
    "\n",
    "### Task 2: Exploratory Data Analysis (EDA)\n",
    "The candidate should perform exploratory data analysis to understand patterns in the data. The tasks may include:\n",
    "\n",
    "1. **Correlation Analysis**:\n",
    "    - Calculate correlation between numerical features.\n",
    "    - Visualize the correlation matrix and discuss significant correlations.\n",
    "\n",
    "2. **Distribution of Target Variable**:\n",
    "    - Analyze the distribution of the target variable (e.g., for classification, see the class distribution).\n",
    "    - Visualize using histograms, box plots, or violin plots.\n",
    "\n",
    "3. **Feature Importance (Initial) with Visualization**:\n",
    "    - For classification or regression tasks, calculate feature importance using techniques such as `RandomForestClassifier` or `GradientBoostingClassifier` and visualize the top features.\n",
    "\n",
    "### Task 3: Predictive Modeling\n",
    "1. **Model Selection**:\n",
    "    - Split the dataset into training and test sets.\n",
    "    - Train different models on the training set (e.g., **Logistic Regression**, **Random Forest**, **XGBoost**).\n",
    "    - Tune hyperparameters using cross-validation for one of the models.\n",
    "\n",
    "2. **Performance Evaluation**:\n",
    "    - Evaluate the models using appropriate metrics (e.g., **accuracy**, **precision**, **recall**, **F1 score** for classification; **RMSE**, **MAE** for regression).\n",
    "    - Visualize model performance through confusion matrices (for classification) or residual plots (for regression).\n",
    "    - Discuss potential overfitting and how to avoid it.\n",
    "\n",
    "3. **Model Interpretation**:\n",
    "    - Use **SHAP** (SHapley Additive exPlanations) or **LIME** (Local Interpretable Model-agnostic Explanations) to interpret the model predictions and understand which features contribute most to the model's decisions.\n",
    "\n",
    "### Python Code Template\n",
    "\n",
    "```python\n",
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import shap\n",
    "\n",
    "# Task 1: Data Loading and Initial Exploration\n",
    "def load_and_explore_data(filepath):\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(\"Dataset Shape:\", df.shape)\n",
    "    # explore dataset\n",
    "    print(\"Summary of the dataset:\\n\", df.info())\n",
    "    print(\"Basic Statistics:\\n\", df.describe())\n",
    "    return df\n",
    "\n",
    "# Task 2: Data Cleaning\n",
    "def clean_data(df):\n",
    "    # Handle missing values (Imputation example: fill missing values with mean for numeric features)\n",
    "    \n",
    "    # Handle outliers (example: remove rows with values outside of 3 standard deviations)\n",
    "    return df\n",
    "\n",
    "# Task 3: Feature Engineering\n",
    "def feature_engineering(df):\n",
    "    # Encode categorical features\n",
    "    # Feature scaling\n",
    "    return df\n",
    "\n",
    "# Task 4: EDA - Correlation Analysis\n",
    "def correlation_analysis(df):\n",
    "    corr_matrix = df.corr()\n",
    "    plt.figure(figsize=(12,8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "    plt.title(\"Correlation Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Task 5: Model Training and Performance Evaluation\n",
    "def model_training(df, target_column):\n",
    "    # Split data into features and target\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Train-Test Split\n",
    "        \n",
    "    # Train a RandomForest Model\n",
    "        \n",
    "    # Model Prediction and Evaluation\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    \n",
    "    # Feature Importance Visualization\n",
    "    \n",
    "# Task 6: Model Interpretation using SHAP\n",
    "def shap_interpretation(model, X_train):\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_train)\n",
    "    shap.summary_plot(shap_values[1], X_train)\n",
    "\n",
    "# Example workflow\n",
    "if __name__ == \"__main__\":\n",
    "    filepath = \"your_dataset.csv\"\n",
    "    target_column = \"your_target_column\"\n",
    "    \n",
    "    df = load_and_explore_data(filepath)\n",
    "    df = clean_data(df)\n",
    "    df = feature_engineering(df)\n",
    "    correlation_analysis(df)\n",
    "    model = model_training(df, target_column)\n",
    "    shap_interpretation(model, df.drop(target_column, axis=1))\n",
    "```\n",
    "\n",
    "### Test Explanation:\n",
    "\n",
    "1. **Data Loading and Exploration**: The candidate should demonstrate how to load the dataset, perform basic summary statistics, and visualize features.\n",
    "2. **Data Cleaning**: The candidate handles missing values and outliers.\n",
    "3. **Feature Engineering**: The candidate encodes categorical features and scales the dataset.\n",
    "4. **EDA and Correlation Analysis**: The candidate should analyze relationships between features.\n",
    "5. **Model Training**: The candidate trains a **RandomForest** model, evaluates its performance using standard metrics, and visualizes feature importance.\n",
    "6. **Model Interpretation**: The candidate uses **SHAP** to interpret the modelâ€™s decision-making process.\n",
    "\n",
    "This test ensures that the candidate can handle the end-to-end process of data analysis, modeling, and model interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a6438b-5504-440a-930a-ae580bf2288d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
