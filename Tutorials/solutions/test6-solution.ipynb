{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0da47cbc-41fb-4455-ba93-16f7b7fd8ab1",
   "metadata": {},
   "source": [
    "# Apple Lead Data Scientist Test - Solution\n",
    "\n",
    "This document presents a complete solution to the Apple Lead Data Scientist Test. In this solution, we:\n",
    "- Load and preprocess the synthetic dataset (`apple_data.csv`)\n",
    "- Conduct exploratory data analysis (EDA)\n",
    "- Perform feature engineering\n",
    "- Build a classification model to predict purchase likelihood\n",
    "- Tune the model using GridSearchCV\n",
    "- Discuss deployment considerations and business implications\n",
    "\n",
    "> **Note:** The synthetic dataset can be generated using the provided dataset generation script. Ensure that `apple_data.csv` is in your working directory before running this solution.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Data Loading and Preprocessing](#data-loading-and-preprocessing)\n",
    "2. [Exploratory Data Analysis (EDA)](#exploratory-data-analysis-eda)\n",
    "3. [Feature Engineering](#feature-engineering)\n",
    "4. [Model Building](#model-building)\n",
    "5. [Model Tuning and Evaluation](#model-tuning-and-evaluation)\n",
    "6. [Deployment Considerations and Business Context](#deployment-considerations-and-business-context)\n",
    "7. [Conclusion](#conclusion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcdd1c3e-4210-485a-b21c-5c46d64d61e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset:\n",
      "   session_id  user_id            timestamp device_type  action    price  \\\n",
      "0           1      103  2023-06-22 21:36:51      iPhone    view  1692.80   \n",
      "1           2      436  2023-05-10 00:41:01      iPhone    view  2988.78   \n",
      "2           3      349  2023-04-05 03:58:11        iPad    view  2441.30   \n",
      "3           4      271  2023-04-25 13:00:04      iPhone   click  2567.64   \n",
      "4           5      107  2023-08-31 21:22:51         Mac  browse  1978.38   \n",
      "\n",
      "  product_category  purchase  \n",
      "0       smartphone         0  \n",
      "1       smartphone         0  \n",
      "2           tablet         0  \n",
      "3       smartphone         1  \n",
      "4           laptop         0  \n",
      "\n",
      "Missing values in each column:\n",
      "session_id          0\n",
      "user_id             0\n",
      "timestamp           0\n",
      "device_type         0\n",
      "action              0\n",
      "price               0\n",
      "product_category    0\n",
      "purchase            0\n",
      "hour                0\n",
      "day_of_week         0\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "session_id                   int64\n",
      "user_id                      int64\n",
      "timestamp           datetime64[ns]\n",
      "device_type                 object\n",
      "action                      object\n",
      "price                      float64\n",
      "product_category            object\n",
      "purchase                     int64\n",
      "hour                         int32\n",
      "day_of_week                  int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('../../data/apple_data.csv')\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "\n",
    "# Create time-based features: hour of day and day of week\n",
    "data['hour'] = data['timestamp'].dt.hour\n",
    "data['day_of_week'] = data['timestamp'].dt.dayofweek\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Display data types after conversion\n",
    "print(\"\\nData types:\")\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd20394-d29c-4adc-877d-2b9ccfc52f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Summary statistics:\")\n",
    "print(data.describe())\n",
    "\n",
    "# Count plot for purchase indicator\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='purchase', data=data)\n",
    "plt.title(\"Distribution of Purchase Indicator\")\n",
    "plt.xlabel(\"Purchase (0 = No, 1 = Yes)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Histogram for price distribution\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(data['price'], bins=30, kde=True)\n",
    "plt.title(\"Price Distribution\")\n",
    "plt.xlabel(\"Price ($)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Count plot for device type vs. purchase\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(x='device_type', hue='purchase', data=data)\n",
    "plt.title(\"Device Type vs Purchase\")\n",
    "plt.xlabel(\"Device Type\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Box plot for price by product category\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(x='product_category', y='price', data=data)\n",
    "plt.title(\"Price Distribution by Product Category\")\n",
    "plt.xlabel(\"Product Category\")\n",
    "plt.ylabel(\"Price ($)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175cf3a5-3e0f-4828-aaad-27bd1a6f20fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables: device_type, action, and product_category\n",
    "data_encoded = pd.get_dummies(data, columns=['device_type', 'action', 'product_category'], drop_first=True)\n",
    "\n",
    "# Verify the encoded columns\n",
    "print(\"Columns after one-hot encoding:\")\n",
    "print(data_encoded.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7204372e-b8e7-4f94-83d9-d35c09468d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "# Drop columns that are identifiers or not useful for prediction\n",
    "X = data_encoded.drop(columns=['purchase', 'session_id', 'user_id', 'timestamp'])\n",
    "y = data_encoded['purchase']\n",
    "\n",
    "# Split the data (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train a Random Forest model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Model Accuracy:\", accuracy)\n",
    "print(\"ROC AUC Score:\", roc_auc)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e92b55a-56ef-418d-bbce-95bc1e4ba60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a grid of hyperparameters\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV to optimize for ROC AUC score\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42),\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters and corresponding ROC AUC score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validated ROC AUC:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "best_accuracy = accuracy_score(y_test, y_pred_best)\n",
    "best_roc_auc = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(\"\\nBest Model Accuracy:\", best_accuracy)\n",
    "print(\"Best Model ROC AUC Score:\", best_roc_auc)\n",
    "\n",
    "# Visualize feature importance from the best model\n",
    "importances = best_model.feature_importances_\n",
    "features = X.columns\n",
    "feature_importance = pd.Series(importances, index=features).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=feature_importance.values, y=feature_importance.index)\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19904005-7ad0-42dd-9cc4-8a584c4f0711",
   "metadata": {},
   "source": [
    "Deployment Considerations and Business Context\n",
    "Deployment Considerations\n",
    "Model Serving:\n",
    "Deploy the model using RESTful APIs with frameworks like Flask or FastAPI. For scalability, consider cloud platforms (e.g., AWS SageMaker, Google AI Platform).\n",
    "\n",
    "Monitoring:\n",
    "Implement monitoring for performance metrics and data drift using tools such as Prometheus or cloud-native monitoring services.\n",
    "\n",
    "Retraining:\n",
    "Develop an automated pipeline for model retraining when performance degrades due to changes in the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c6ea70-ec6a-4dd2-9ecb-d4b3794b6fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
