{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bc0deac-f93a-410c-a509-9e206a3e12be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five rows of the dataset:\n",
      "   session_id  user_id            timestamp       action   price  \\\n",
      "0           1      103  2022-11-01 10:00:41  add_to_cart  187.89   \n",
      "1           2      271  2022-10-13 18:14:41        click   27.43   \n",
      "2           3      107  2022-05-24 22:39:31         view  178.22   \n",
      "3           4       72  2022-01-19 18:11:21        click  465.00   \n",
      "4           5      189  2022-01-18 19:02:56         view  192.72   \n",
      "\n",
      "  product_category  purchase  \n",
      "0          Fashion         0  \n",
      "1             Toys         0  \n",
      "2             Toys         0  \n",
      "3      Electronics         0  \n",
      "4          Fashion         0  \n",
      "\n",
      "Missing values in each column:\n",
      "session_id          0\n",
      "user_id             0\n",
      "timestamp           0\n",
      "action              0\n",
      "price               0\n",
      "product_category    0\n",
      "purchase            0\n",
      "dtype: int64\n",
      "\n",
      "Data types after conversion:\n",
      "session_id                   int64\n",
      "user_id                      int64\n",
      "timestamp           datetime64[ns]\n",
      "action                      object\n",
      "price                      float64\n",
      "product_category            object\n",
      "purchase                     int64\n",
      "hour                         int32\n",
      "day_of_week                  int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('../../data/ebay_data.csv')\n",
    "print(\"First five rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Convert the 'timestamp' column to a datetime object\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "\n",
    "# Create new time-based features: hour of day and day of week\n",
    "data['hour'] = data['timestamp'].dt.hour\n",
    "data['day_of_week'] = data['timestamp'].dt.dayofweek\n",
    "\n",
    "# Display the updated DataFrame structure\n",
    "print(\"\\nData types after conversion:\")\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dfc82a-236f-4aea-8245-2ba10bd7dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numerical features\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(data.describe())\n",
    "\n",
    "# Plot the distribution of the target variable 'purchase'\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='purchase', data=data)\n",
    "plt.title(\"Distribution of Purchase Indicator\")\n",
    "plt.xlabel(\"Purchase (0 = No, 1 = Yes)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Visualize the distribution of 'price'\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(data['price'], bins=30, kde=True)\n",
    "plt.title(\"Price Distribution\")\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Visualize purchase counts by product category\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(x='product_category', hue='purchase', data=data)\n",
    "plt.title(\"Purchase by Product Category\")\n",
    "plt.xlabel(\"Product Category\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df7c8b3-4def-4a0c-b0e1-617c4fa0aca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables: 'action' and 'product_category'\n",
    "data_encoded = pd.get_dummies(data, columns=['action', 'product_category'], drop_first=True)\n",
    "\n",
    "# Display the columns after encoding to verify\n",
    "print(\"\\nColumns after one-hot encoding:\")\n",
    "print(data_encoded.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de403d31-6358-40a7-8701-05ae2998f981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "# Remove non-informative columns: session_id, user_id, timestamp\n",
    "X = data_encoded.drop(columns=['purchase', 'session_id', 'user_id', 'timestamp'])\n",
    "y = data_encoded['purchase']\n",
    "\n",
    "# Split the dataset into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a Random Forest model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Model Accuracy:\", accuracy)\n",
    "print(\"ROC AUC Score:\", roc_auc)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b750916-c7f6-4f7d-92d5-d7cb51c418c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a grid of hyperparameters for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV to optimize for ROC AUC score\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output the best parameters and the best cross-validated ROC AUC score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validated ROC AUC:\", grid_search.best_score_)\n",
    "\n",
    "# Use the best estimator to make predictions\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "print(\"\\nBest Model Accuracy:\", accuracy_score(y_test, y_pred_best))\n",
    "print(\"Best Model ROC AUC Score:\", roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1]))\n",
    "\n",
    "# Plot feature importances from the best model\n",
    "importances = best_model.feature_importances_\n",
    "features = X.columns\n",
    "feature_importance = pd.Series(importances, index=features).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=feature_importance.values, y=feature_importance.index)\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c395f-0d1b-42fe-b32c-94ccf4432ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "# Define a custom evaluation metric: F-beta score with beta=0.5 (emphasizing precision)\n",
    "def custom_metric(y_true, y_pred):\n",
    "    return fbeta_score(y_true, y_pred, beta=0.5)\n",
    "\n",
    "custom_score = custom_metric(y_test, y_pred_best)\n",
    "print(\"Custom Evaluation Metric (F0.5 Score):\", custom_score)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d158040-891a-4a35-b14d-ecf78cba7399",
   "metadata": {},
   "source": [
    "Deployment Considerations\n",
    "Discussion:\n",
    "\n",
    "Model Serving: The model could be deployed using RESTful APIs with frameworks such as Flask or FastAPI. For scalability, consider cloud platforms like AWS SageMaker or Google AI Platform.\n",
    "Monitoring: Implement performance and data drift monitoring with tools like Prometheus or cloud-native solutions.\n",
    "Retraining Strategy: Establish automated retraining pipelines to respond to degradation in model performance over time.\n",
    "Scalability: Use containerization (Docker) and orchestration (Kubernetes) to manage the production environment effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3ae59a-5fac-4e65-8796-a92b942bc2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f580dec0-d81a-4337-8407-e44fcd71dd6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
